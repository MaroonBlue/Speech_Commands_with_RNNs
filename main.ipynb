{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNHwZY9ODkDOKZJIcSo9u8L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaroonBlue/Speech_Commands_with_RNNs/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Environment setup"
      ],
      "metadata": {
        "id": "Si2Uk1xlJmqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cloning repository"
      ],
      "metadata": {
        "id": "U3yOj5hKKM2r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FaE53h2rG-9Y",
        "outputId": "6cd0aeb7-548c-4999-970e-f10a6465143d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'Speech_Commands_with_RNNs/': No such file or directory\n",
            "Cloning into 'Speech_Commands_with_RNNs'...\n",
            "remote: Enumerating objects: 84, done.\u001b[K\n",
            "remote: Counting objects: 100% (84/84), done.\u001b[K\n",
            "remote: Compressing objects: 100% (61/61), done.\u001b[K\n",
            "remote: Total 84 (delta 36), reused 46 (delta 15), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (84/84), 1.11 MiB | 5.16 MiB/s, done.\n",
            "/content/Speech_Commands_with_RNNs\n"
          ]
        }
      ],
      "source": [
        "!rm -r Speech_Commands_with_RNNs/\n",
        "!git clone https://github.com/MaroonBlue/Speech_Commands_with_RNNs.git\n",
        "%cd Speech_Commands_with_RNNs/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing packages"
      ],
      "metadata": {
        "id": "SC7iDRhkKQ0B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt\n",
        "!pip install -U -q tensorflow tensorflow_datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5dENBAmHRNl",
        "outputId": "cd8383e2-0494-4a4d-d89e-e425dcb482b8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: absl-py==1.4.0 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 1)) (1.4.0)\n",
            "Requirement already satisfied: appdirs==1.4.4 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 2)) (1.4.4)\n",
            "Collecting array-record==0.2.0\n",
            "  Downloading array_record-0.2.0-py39-none-any.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 4)) (1.6.3)\n",
            "Requirement already satisfied: audioread==3.0.0 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 5)) (3.0.0)\n",
            "Requirement already satisfied: cachetools==5.3.0 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 6)) (5.3.0)\n",
            "Requirement already satisfied: certifi==2022.12.7 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 7)) (2022.12.7)\n",
            "Requirement already satisfied: cffi==1.15.1 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 8)) (1.15.1)\n",
            "Collecting charset-normalizer==3.1.0\n",
            "  Downloading charset_normalizer-3.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.2/199.2 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click==8.1.3 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 10)) (8.1.3)\n",
            "Collecting colorama==0.4.6\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Collecting decorator==5.1.1\n",
            "  Downloading decorator-5.1.1-py3-none-any.whl (9.1 kB)\n",
            "Requirement already satisfied: dm-tree==0.1.8 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 13)) (0.1.8)\n",
            "Collecting docker-pycreds==0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting EasyProcess==1.1\n",
            "  Downloading EasyProcess-1.1-py3-none-any.whl (8.7 kB)\n",
            "Collecting einops==0.6.1\n",
            "  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting entrypoint2==1.1\n",
            "  Downloading entrypoint2-1.1-py2.py3-none-any.whl (9.9 kB)\n",
            "Requirement already satisfied: etils==1.2.0 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 18)) (1.2.0)\n",
            "Requirement already satisfied: flatbuffers==23.3.3 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 19)) (23.3.3)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 20)) (0.4.0)\n",
            "Collecting gitdb==4.0.10\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting GitPython==3.1.31\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-auth==2.17.3\n",
            "  Downloading google_auth-2.17.3-py2.py3-none-any.whl (178 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.2/178.2 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-auth-oauthlib==1.0.0 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 24)) (1.0.0)\n",
            "Requirement already satisfied: google-pasta==0.2.0 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 25)) (0.2.0)\n",
            "Requirement already satisfied: googleapis-common-protos==1.59.0 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 26)) (1.59.0)\n",
            "Collecting grpcio==1.54.0\n",
            "  Downloading grpcio-1.54.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: h5py==3.8.0 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 28)) (3.8.0)\n",
            "Requirement already satisfied: idna==3.4 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 29)) (3.4)\n",
            "Collecting importlib-metadata==6.5.0\n",
            "  Downloading importlib_metadata-6.5.0-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: importlib-resources==5.12.0 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 31)) (5.12.0)\n",
            "Requirement already satisfied: jax==0.4.8 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 32)) (0.4.8)\n",
            "Requirement already satisfied: joblib==1.2.0 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 33)) (1.2.0)\n",
            "Requirement already satisfied: keras==2.12.0 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 34)) (2.12.0)\n",
            "Requirement already satisfied: lazy_loader==0.2 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 35)) (0.2)\n",
            "Requirement already satisfied: libclang==16.0.0 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 36)) (16.0.0)\n",
            "Requirement already satisfied: librosa==0.10.0.post2 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 37)) (0.10.0.post2)\n",
            "Requirement already satisfied: llvmlite==0.39.1 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 38)) (0.39.1)\n",
            "Requirement already satisfied: Markdown==3.4.3 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 39)) (3.4.3)\n",
            "Requirement already satisfied: MarkupSafe==2.1.2 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 40)) (2.1.2)\n",
            "Collecting ml-dtypes==0.1.0\n",
            "  Downloading ml_dtypes-0.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (191 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m191.1/191.1 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: msgpack==1.0.5 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 42)) (1.0.5)\n",
            "Requirement already satisfied: numba==0.56.4 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 43)) (0.56.4)\n",
            "Collecting numpy==1.23.5\n",
            "  Downloading numpy-1.23.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: oauthlib==3.2.2 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 45)) (3.2.2)\n",
            "Requirement already satisfied: opt-einsum==3.3.0 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 46)) (3.3.0)\n",
            "Collecting packaging==23.1\n",
            "  Downloading packaging-23.1-py3-none-any.whl (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pandas==2.0.0\n",
            "  Downloading pandas-2.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pathtools==0.1.2\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting patool==1.12\n",
            "  Downloading patool-1.12-py2.py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.5/77.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pooch==1.6.0 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 51)) (1.6.0)\n",
            "Requirement already satisfied: promise==2.3 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 52)) (2.3)\n",
            "Collecting protobuf==4.22.3\n",
            "  Downloading protobuf-4.22.3-cp37-abi3-manylinux2014_x86_64.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.4/302.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting psutil==5.9.5\n",
            "  Downloading psutil-5.9.5-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (282 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m282.1/282.1 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyasn1==0.4.8 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 55)) (0.4.8)\n",
            "Requirement already satisfied: pyasn1-modules==0.2.8 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 56)) (0.2.8)\n",
            "Requirement already satisfied: pycparser==2.21 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 57)) (2.21)\n",
            "Requirement already satisfied: python-dateutil==2.8.2 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 58)) (2.8.2)\n",
            "Collecting pytz==2023.3\n",
            "  Downloading pytz-2023.3-py2.py3-none-any.whl (502 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m502.3/502.3 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyunpack==0.3\n",
            "  Downloading pyunpack-0.3-py2.py3-none-any.whl (4.1 kB)\n",
            "Requirement already satisfied: PyYAML==6.0 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 61)) (6.0)\n",
            "Collecting requests==2.28.2\n",
            "  Downloading requests-2.28.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests-oauthlib==1.3.1 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 63)) (1.3.1)\n",
            "Requirement already satisfied: rsa==4.9 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 64)) (4.9)\n",
            "Requirement already satisfied: scikit-learn==1.2.2 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 65)) (1.2.2)\n",
            "Requirement already satisfied: scipy==1.10.1 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 66)) (1.10.1)\n",
            "Collecting sentry-sdk==1.20.0\n",
            "  Downloading sentry_sdk-1.20.0-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.8/198.8 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting setproctitle==1.3.2\n",
            "  Downloading setproctitle-1.3.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: six==1.16.0 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 69)) (1.16.0)\n",
            "Collecting smmap==5.0.0\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: soundfile==0.12.1 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 71)) (0.12.1)\n",
            "Requirement already satisfied: soxr==0.3.5 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 72)) (0.3.5)\n",
            "Requirement already satisfied: termcolor==2.2.0 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 73)) (2.2.0)\n",
            "Requirement already satisfied: threadpoolctl==3.1.0 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 74)) (3.1.0)\n",
            "Requirement already satisfied: toml==0.10.2 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 75)) (0.10.2)\n",
            "Requirement already satisfied: tqdm==4.65.0 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 76)) (4.65.0)\n",
            "Requirement already satisfied: typing_extensions==4.5.0 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 77)) (4.5.0)\n",
            "Requirement already satisfied: tzdata==2023.3 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 78)) (2023.3)\n",
            "Requirement already satisfied: urllib3==1.26.15 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 79)) (1.26.15)\n",
            "Collecting wandb==0.14.2\n",
            "  Downloading wandb-0.14.2-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Werkzeug==2.2.3 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 81)) (2.2.3)\n",
            "Requirement already satisfied: wrapt==1.14.1 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 82)) (1.14.1)\n",
            "Requirement already satisfied: zipp==3.15.0 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 83)) (3.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse==1.6.3->-r requirements.txt (line 4)) (0.40.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from numba==0.56.4->-r requirements.txt (line 43)) (67.6.1)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8807 sha256=d5d3e80f128481078d5d7a0e565956b2026053bae2f40152a9bce992c9bda1ac\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/0a/67/ada2a22079218c75a88361c0782855cc72aebc4d18d0289d05\n",
            "Successfully built pathtools\n",
            "Installing collected packages: pytz, patool, pathtools, entrypoint2, EasyProcess, smmap, setproctitle, sentry-sdk, pyunpack, psutil, protobuf, packaging, numpy, importlib-metadata, grpcio, einops, docker-pycreds, decorator, colorama, charset-normalizer, requests, pandas, ml-dtypes, google-auth, gitdb, GitPython, array-record, wandb\n",
            "  Attempting uninstall: pytz\n",
            "    Found existing installation: pytz 2022.7.1\n",
            "    Uninstalling pytz-2022.7.1:\n",
            "      Successfully uninstalled pytz-2022.7.1\n",
            "  Attempting uninstall: psutil\n",
            "    Found existing installation: psutil 5.9.4\n",
            "    Uninstalling psutil-5.9.4:\n",
            "      Successfully uninstalled psutil-5.9.4\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 23.0\n",
            "    Uninstalling packaging-23.0:\n",
            "      Successfully uninstalled packaging-23.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.22.4\n",
            "    Uninstalling numpy-1.22.4:\n",
            "      Successfully uninstalled numpy-1.22.4\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 6.3.0\n",
            "    Uninstalling importlib-metadata-6.3.0:\n",
            "      Successfully uninstalled importlib-metadata-6.3.0\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.53.0\n",
            "    Uninstalling grpcio-1.53.0:\n",
            "      Successfully uninstalled grpcio-1.53.0\n",
            "  Attempting uninstall: decorator\n",
            "    Found existing installation: decorator 4.4.2\n",
            "    Uninstalling decorator-4.4.2:\n",
            "      Successfully uninstalled decorator-4.4.2\n",
            "  Attempting uninstall: charset-normalizer\n",
            "    Found existing installation: charset-normalizer 2.0.12\n",
            "    Uninstalling charset-normalizer-2.0.12:\n",
            "      Successfully uninstalled charset-normalizer-2.0.12\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.27.1\n",
            "    Uninstalling requests-2.27.1:\n",
            "      Successfully uninstalled requests-2.27.1\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.5.3\n",
            "    Uninstalling pandas-1.5.3:\n",
            "      Successfully uninstalled pandas-1.5.3\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.0.4\n",
            "    Uninstalling ml-dtypes-0.0.4:\n",
            "      Successfully uninstalled ml-dtypes-0.0.4\n",
            "  Attempting uninstall: google-auth\n",
            "    Found existing installation: google-auth 2.17.2\n",
            "    Uninstalling google-auth-2.17.2:\n",
            "      Successfully uninstalled google-auth-2.17.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "tensorflow-metadata 1.13.0 requires protobuf<4,>=3.13, but you have protobuf 4.22.3 which is incompatible.\n",
            "moviepy 1.0.3 requires decorator<5.0,>=4.0.2, but you have decorator 5.1.1 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas~=1.5.3, but you have pandas 2.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed EasyProcess-1.1 GitPython-3.1.31 array-record-0.2.0 charset-normalizer-3.1.0 colorama-0.4.6 decorator-5.1.1 docker-pycreds-0.4.0 einops-0.6.1 entrypoint2-1.1 gitdb-4.0.10 google-auth-2.17.3 grpcio-1.54.0 importlib-metadata-6.5.0 ml-dtypes-0.1.0 numpy-1.23.5 packaging-23.1 pandas-2.0.0 pathtools-0.1.2 patool-1.12 protobuf-4.22.3 psutil-5.9.5 pytz-2023.3 pyunpack-0.3 requests-2.28.2 sentry-sdk-1.20.0 setproctitle-1.3.2 smmap-5.0.0 wandb-0.14.2\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading dataset"
      ],
      "metadata": {
        "id": "1wkIdcudKXIa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-YZwwC0Bdv_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python utils/download_dataset.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rew8dJE4HwHs",
        "outputId": "bc586dff-662e-459a-f38e-fd810ca19823"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-04-19 13:22:55.274279: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-04-19 13:22:56.700841: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Downloading data from https://storage.googleapis.com/kagglesdsdata/competitions/7634/46676/train.7z?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1682110428&Signature=cq%2FPQDH8watXYiVGtl2xPd0cRyHkS3F4QyPYFhn6GtyZ85MJueaOUnWhcn1lzww%2BCkwbUvDXhrpqJ4EYmh%2F6tsicinc%2FC08ZyvZ1SkPDC26yQvSGsLvjuX5PVeNT2ucFL1WaFUlXsyHX9KbqgsOeNgqO5nzsLAyJLTyLDrTKsQmxq4v4r8ryAgrXB4NZCQR%2BYeZJEU8A2oWvmkHo9vtaa7xPtSmqs%2Flm2bKvQq3Fjcl8r8%2BTFJwjBDwgfSFKzpD8QJ4OXOh2x%2FEDVptrJAbQ5LiH5K8JydEnWBtyBLUHxTJg%2Bl5pH%2Flku1votcwMMFpHFPZl3u5PopYfq%2Fye6%2FxfwA%3D%3D&response-content-disposition=attachment%3B+filename%3Dtrain.7z\n",
            "1121103842/1121103842 [==============================] - 9s 0us/step\n",
            "Moved 6798 files to the validation directory\n",
            "Moved 6835 files to the test directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiments"
      ],
      "metadata": {
        "id": "hqJHBPmzOLRp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from os.path import isdir, join\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "import random\n",
        "import copy\n",
        "import librosa\n",
        "\n",
        "np.seterr(all=\"ignore\")\n",
        "\n",
        "def get_one_noise(background_noise, noise_num = 0, sample_rate=16000):\n",
        "    selected_noise = background_noise[noise_num]\n",
        "    start_idx = random.randint(0, len(selected_noise)- 1 - sample_rate)\n",
        "    return selected_noise[start_idx:(start_idx + sample_rate)]\n",
        "\n",
        "def make_train_dataset(path='./data/train/audio/', sample_rate=16000, augment = 1):\n",
        "    train_audio_path = path\n",
        "    dirs = [f for f in os.listdir(train_audio_path) if isdir(join(train_audio_path, f))]\n",
        "    dirs.sort()\n",
        "    \n",
        "    all_wav = []\n",
        "    unknown_wav = []\n",
        "    label_all = []\n",
        "    label_value = {}\n",
        "    target_list = ['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go']\n",
        "    unknown_list = [d for d in dirs if d not in target_list and d != '_background_noise_' ]\n",
        "\n",
        "    i = 0\n",
        "    background = [f for f in os.listdir(join(train_audio_path, '_background_noise_')) if f.endswith('.wav')]\n",
        "    background_noise = []\n",
        "    for wav in background : \n",
        "        samples, sample_rate = librosa.load(join(join(train_audio_path,'_background_noise_'),wav), sr = sample_rate)\n",
        "        background_noise.append(samples)\n",
        "\n",
        "    for direct in dirs[1:]:\n",
        "        waves = [f for f in os.listdir(join(train_audio_path, direct)) if f.endswith('.wav')]\n",
        "        label_value[direct] = i\n",
        "        i = i + 1\n",
        "        for wav in waves:\n",
        "            samples, sample_rate = librosa.load(join(join(train_audio_path,direct),wav), sr = sample_rate)\n",
        "            if len(samples) != sample_rate : \n",
        "                continue\n",
        "            if direct in unknown_list:\n",
        "                unknown_wav.append(samples)\n",
        "            else:\n",
        "                label_all.append(direct)\n",
        "                all_wav.append([samples, direct])\n",
        "    \n",
        "    wav_all = np.reshape(np.delete(all_wav,1,1),(len(all_wav)))\n",
        "    label_all = [i for i in np.delete(all_wav,0,1).tolist()]\n",
        "\n",
        "    max_ratio = 0.1\n",
        "    noised_wav = []\n",
        "    delete_index = []\n",
        "    for i in range(augment):\n",
        "        noise = get_one_noise(background_noise, i)\n",
        "        for i, s in enumerate(wav_all):\n",
        "            if len(s) != sample_rate:\n",
        "                delete_index.append(i)\n",
        "                continue\n",
        "            s = s + (max_ratio * noise)\n",
        "            noised_wav.append(s)\n",
        "    np.delete(wav_all, delete_index)\n",
        "    np.delete(label_all, delete_index)\n",
        "    \n",
        "    wav_vals = np.array([x for x in wav_all])\n",
        "    label_vals = [x for x in label_all]\n",
        "\n",
        "    labels = copy.deepcopy(label_vals)\n",
        "    for _ in range(augment):\n",
        "        label_vals = np.concatenate((label_vals, labels), axis = 0)\n",
        "    label_vals = label_vals.reshape(-1,1)\n",
        "\n",
        "    unknown = unknown_wav\n",
        "    np.random.shuffle(unknown_wav)\n",
        "    unknown = np.array(unknown)\n",
        "    unknown = unknown[:2000*(augment+1)]\n",
        "    unknown_label = np.array(['unknown' for _ in range(2000*(augment+1))])\n",
        "    unknown_label = unknown_label.reshape(2000*(augment+1),1)\n",
        "\n",
        "    delete_index = []\n",
        "    for i,w in enumerate(unknown):\n",
        "        if len(w) != sample_rate:\n",
        "            delete_index.append(i)\n",
        "    unknown = np.delete(unknown, delete_index, axis=0)\n",
        "\n",
        "    silence_wav = []\n",
        "    num_wav = (2000*(augment+1))//len(background_noise)\n",
        "    for i, _ in enumerate(background_noise):\n",
        "        for _ in range((2000*(augment+1))//len(background_noise)):\n",
        "            silence_wav.append(get_one_noise(background_noise, i))\n",
        "    silence_wav = np.array(silence_wav)\n",
        "    silence_label = np.array(['silence' for _ in range(num_wav*len(background_noise))])\n",
        "    silence_label = silence_label.reshape(-1,1)\n",
        "\n",
        "    wav_vals = np.reshape(wav_vals, (-1, sample_rate))\n",
        "    noised_wav = np.reshape(noised_wav, (-1, sample_rate))\n",
        "    unknown = np.reshape(unknown, (-1, sample_rate))\n",
        "    silence_wav = np.reshape(silence_wav, (-1, sample_rate))\n",
        "\n",
        "    # print(wav_vals.shape)\n",
        "    # print(noised_wav.shape)\n",
        "    # print(unknown.shape)\n",
        "    # print(silence_wav.shape)\n",
        "\n",
        "    # print(label_vals.shape)\n",
        "    # print(unknown_label.shape)\n",
        "    # print(silence_label.shape)\n",
        "\n",
        "    wav_vals = np.concatenate((wav_vals, noised_wav), axis = 0)\n",
        "    wav_vals = np.concatenate((wav_vals, unknown), axis = 0)\n",
        "    wav_vals = np.concatenate((wav_vals, silence_wav), axis = 0)\n",
        "\n",
        "    label_vals = np.concatenate((label_vals, unknown_label), axis = 0)\n",
        "    label_vals = np.concatenate((label_vals, silence_label), axis = 0)\n",
        "\n",
        "    train_data = wav_vals\n",
        "    train_label = label_vals\n",
        "\n",
        "    assert(len(wav_vals) == len(label_vals))\n",
        "\n",
        "    label_value = target_list\n",
        "    label_value.append('unknown')\n",
        "    label_value.append('silence')\n",
        "    new_label_value = dict()\n",
        "    for i, l in enumerate(label_value):\n",
        "        new_label_value[l] = i\n",
        "    label_value = new_label_value\n",
        "\n",
        "    temp = []\n",
        "    for v in train_label:\n",
        "        temp.append(label_value[v[0]])\n",
        "    train_labels = np.array(temp)\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((train_data, train_labels))\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "Lyai6akROPuw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = make_train_dataset()"
      ],
      "metadata": {
        "id": "s3lg-QGfsxl3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}